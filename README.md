# From NLP to CSS: A Practical Tutorial on using Transformers in your Research
_This page is an open-repository that provides you with the material from our tutorial on Transformers ðŸ¤–, HuggingFace ðŸ¤— and Social Science Applications ðŸ‘¥
which was presented by [Christopher Klamm](https://linktr.ee/chkla), [Moritz Laurer](https://www.ceps.eu/ceps-staff/moritz-laurer/) and [Elliott Ash](https://elliottash.com) on the [7th International Conference on Computational Social Science](https://ic2s2-2021.ethz.ch)._

## Overview
Transformers have revolutionised Natural Language Processing (NLP) since 2018. 
From text classification, to text summarization, to translation â€“ the state of the 
art for most NLP tasks today is dominated by this type of new deep learning model 
architecture. This tutorial will introduce the participants to this new type of 
model; will demonstrate how thousands of pre-trained models can easily be 
used with a few lines of code; outline how they can be used as 
powerful measurement tools in social science using concrete examples 
and dive deeper into the technical ingredients of transformers. 
A key part of the tutorial will be the [HuggingFace](huggingface.com) Transformers library and its 
open-source community. We hope to light the passion of the text-as-data community 
to contribute to and benefit from open source transformers and create closer ties 
with the NLP community.

## Structure
1. Introduction w/o math on transformer-based Language Models [[Slides](https://drive.google.com/file/d/1hQtpWBtDo1GwQJQqOfPeByVTat4UcQhL/view?usp=sharing)] _(8.8.21, updated compact version)_
2. HuggingFace and key NLP tasks and pipeline [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1K9zkPIUBPCWaVgg4duuYKirOrxAID0wG?usp=sharing)
3. Core transformers classes: tokenizers and models [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1K9zkPIUBPCWaVgg4duuYKirOrxAID0wG?usp=sharing)
4. Programming tutorial (Python) to train your first model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1vXgjybT0wMIuSV-J5Xt_WCQ0fKupa1QI?usp=sharing)
5. Theoretical background on transformer-based Language Models [[Slides](https://drive.google.com/file/d/1Wg5EAtu16Sd12IcfLqDWzOlVeoBsjcxp/view?usp=sharing)]
6. Social Science applications [[Slides](https://drive.google.com/file/d/10JsY6laYGGOlL7NgOehnAEVAhAUsFv_O/view?usp=sharing)]

## Related Resources
There are many other amazing ressources on this topic. To name just a few, here are some links for further research:
* Noah Smith (2021): Language Models: Challenges and Progress [[Video](https://drive.google.com/file/d/18PnZRcHPsLP6co-Nis-q1eJAn_PleZmI/view)]
* Sebastian Ruder (2021): Recent Advances in Language Model Fine-tuning [[Link](https://ruder.io/recent-advances-lm-fine-tuning/)]
* Lena Viota (2021): NLP Course | For You [[Link](https://lena-voita.github.io/nlp_course.html)]
* Pavlos Protopapas, Mark Glickman, and Chris Tanner (2021): CS109b: Advanced Topics in Data Science [[Slides](https://harvard-iacs.github.io/2021-CS109B/)]
* Jay Alammar (2021): Language Processing with BERT: The 3 Minute Intro (Deep learning for NLP) [[Video](https://www.youtube.com/watch?v=ioGry-89gqE&t=2s)]
* Melanie Walsh (2021): BERT for Humanists [[Tutorial(]https://melaniewalsh.github.io/BERT-for-Humanists/)]
* KyungHyun Cho (2020): Language modeling [[Video](https://www.youtube.com/watch?v=3ylAcmHVMv8)]
* Rachel Tatmann (2020): NLP for Developers: BERT [[Video](https://www.youtube.com/watch?v=zMxvS7hD-Ug&t=134s)]
* Peter Bloem (2020): Lecture 12.1 Self-attention [[Video](https://www.youtube.com/watch?v=KmAISyVvE1Y)]
* Jay Alammar (2018): The Illustrated Transformer [[Blog](https://jalammar.github.io/illustrated-transformer/)]

_____
_Note: Don't hesitate to send us a message, if something is broken or if you have further questions._
